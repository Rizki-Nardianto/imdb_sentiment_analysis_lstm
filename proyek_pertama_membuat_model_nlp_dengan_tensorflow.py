# -*- coding: utf-8 -*-
"""proyek-pertama_membuat-model-nlp-dengan-tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12hPhAPaGKPrmy4QeD0hoGIXOcgRJknWB

# Proyek Pertama: Membuat Model NLP dengan TensorFlow

Dataset yang digunakan dalam project ini adalah IMDB Dataset yang dapat diunduh pada [tautan](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) ini.
"""

!nvidia-smi

import re
import pandas as pd
import numpy as np
import math
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('wordnet')
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import tensorflow as tf
import keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

df = pd.read_csv('IMDB Dataset.csv')
df.head()

def remove_tags_for_sentence(string):
    removelist = ""
    result = re.sub('','',string)
    result = re.sub('<br />', '', result)
    result = result.lower()
    return result
df['review']=df['review'].apply(lambda x : remove_tags_for_sentence(x))

df.head()

stop_words = set(stopwords.words('english'))
df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))

w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_kalimat(text):
    kalimat = ""
    for w in w_tokenizer.tokenize(text):
        kalimat = kalimat + lemmatizer.lemmatize(w) + " "
    return kalimat
df['review'] = df.review.apply(lemmatize_kalimat)
df

X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)

sentiment_count_all= df['sentiment'].value_counts()
print("Labels Counts:")
print(sentiment_count_all)

train_sentiment_counts = y_train.value_counts()
print("\nTraining Set Sentiment Counts:")
print(train_sentiment_counts)

test_sentiment_counts = y_test.value_counts()
print("\nTesting Set Sentiment Counts:")
print(test_sentiment_counts)

encoder = LabelEncoder()
encoded_y_train = encoder.fit_transform(y_train)
encoded_y_test = encoder.fit_transform(y_test)

tokenizer = Tokenizer(num_words = 3000, oov_token='x')
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

train_sequences = tokenizer.texts_to_sequences(X_train)
train_padded = pad_sequences(
    train_sequences,
    padding='post',
    maxlen=200,
    truncating='post'
)

test_sequences = tokenizer.texts_to_sequences(X_test)
test_padded = pad_sequences(
    test_sequences,
    padding='post',
    maxlen=200,
    truncating='post'
)

model = keras.Sequential([
    keras.layers.Embedding(3000, 100, input_length=200),
    keras.layers.Bidirectional(keras.layers.LSTM(64)),
    keras.layers.Dense(48, activation='relu'),
    keras.layers.Dense(24, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.95):
      print("\nAkurasi telah mencapai >95%!")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 10
history = model.fit(train_padded,
                    encoded_y_train,
                    epochs=num_epochs,
                    verbose=1,
                    callbacks=[callbacks]
                    )

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()